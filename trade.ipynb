{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLob 系列论文\n",
    "\n",
    "- https://arxiv.org/abs/2403.09267 # Deep Limit Order Book Forecasting\n",
    "- https://arxiv.org/abs/1811.10041 # BDLOB: Bayesian Deep Convolutional Neural Networks for Limit Order Books\n",
    "- https://arxiv.org/abs/1803.06917 # Universal features of price formation in financial markets: perspectives from Deep Learning\n",
    "- https://arxiv.org/abs/2101.07107 # Deep Reinforcement Learning for Active High Frequency Trading\n",
    "- https://arxiv.org/abs/1808.03668 # DeepLOB: Deep Convolutional Neural Networks for Limit Order Books\n",
    "- https://ar5iv.labs.arxiv.org/html/2003.00130 # Transformers for limit order books\n",
    "- https://arxiv.org/html/2303.16532v2 # Graph Portfolio: High-Frequency Factor Predictor via Heterogeneous Continual GNNs\n",
    "- https://arxiv.org/abs/2112.08534 # Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture\n",
    "\n",
    "本项目尝试复现最简单的 CNN + LSTM 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# 避免显存不足\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "# 我不知道这个操作有啥用，但是很多项目都有，我就加上了\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "   try:\n",
    "       for gpu in gpus:\n",
    "           tf.config.experimental.set_memory_growth(gpu, True)\n",
    "       logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "       print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "   except RuntimeError as e:\n",
    "       print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本项目只需要 limit order book 数据\n",
    "\n",
    "是否需要处理行情隔天开盘时的跳空？我觉得是需要的，但是所有的paper都不处理，可能搞学术的不想干脏活累活。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行情数据用 tqsdk 下载的，参考download.py\n",
    "data = pd.read_csv('rb2501_tick.csv')\n",
    "data.rename(columns=lambda x: x.replace('SHFE.rb2501.', ''), inplace=True)\n",
    "col = ['bid_price1', 'bid_volume1', 'ask_price1', 'ask_volume1', 'bid_price2', 'bid_volume2', 'ask_price2', 'ask_volume2', 'bid_price3', 'bid_volume3', 'ask_price3', 'ask_volume3', 'bid_price4', 'bid_volume4', 'ask_price4', 'ask_volume4', 'bid_price5', 'bid_volume5', 'ask_price5', 'ask_volume5']\n",
    "data = data[col]\n",
    "data = data.dropna()\n",
    "gc.collect()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行情label算法，将未来行情分为\n",
    "- 0 上涨\n",
    "- 1 下跌\n",
    "- 2 平稳\n",
    "\n",
    "样本中3个类别是否平衡，决定模型loss函数的选择\n",
    "\n",
    "类别极端不平衡时，需要考虑使用focal loss损失函数\n",
    "\n",
    "！我觉得label算法对模型的影响非常大，值得深入思考构造更巧妙的指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调参的地方\n",
    "spread            = 0.003  # 调节价差阈值，参考螺纹钢行情，价格变动10跳，均价3000，10/3000 = 0.003\n",
    "trade_cost        = 1.002  # 调节交易成本，参考螺纹钢行情，手续费3块钱，买卖一手 3*2/3000 = 0.002\n",
    "look_ahead_period = 120*5  # 未来n个tick\n",
    "\n",
    "short_threhold_low   = 1 - spread*2  # 做空方向止盈价 \n",
    "short_threshold_high = 1 + spread*1  # 做空方向止损价\n",
    "long_threshold_high  = 1 + spread*2  # 做多方向止盈价\n",
    "long_threshold_low   = 1 - spread*1  # 做多方向止损价\n",
    "\n",
    "def generate_trade_signals(bid_price: pd.Series, ask_price: pd.Series) -> pd.Series:\n",
    "    '''\n",
    "    根据未来价格变化大小，生成交易信号\n",
    "    信号:\n",
    "    - 0: 做空\n",
    "    - 1: 做多\n",
    "    - 2: 不操作\n",
    "    '''\n",
    "    signals = []\n",
    "\n",
    "    for index in range(int(len(bid_price) - look_ahead_period)):\n",
    "        # 计算当前做空后，未来的资产变化\n",
    "        if min(ask_price[index: index + look_ahead_period]) <= short_threhold_low * bid_price[index] and max(\n",
    "                ask_price[index: index + look_ahead_period]) >= short_threshold_high * bid_price[index]:\n",
    "            # 未来行情触发止盈止损，先触发止盈，则获利\n",
    "            if np.where(ask_price[index: index + look_ahead_period] <= short_threhold_low * bid_price[index])[0][0] < \\\n",
    "                    np.where(ask_price[index: index + look_ahead_period] >= short_threshold_high * bid_price[index])[0][0]:\n",
    "                short_profit = 2 - short_threhold_low * trade_cost\n",
    "            # 未来行情触发止盈止损，先触发止损，则亏损\n",
    "            else:\n",
    "                short_profit = 2 - short_threshold_high * trade_cost\n",
    "        # 未来行情只触发止盈，获利\n",
    "        elif min(ask_price[index: index + look_ahead_period]) <= short_threhold_low * bid_price[index]:\n",
    "            short_profit = 2 - short_threhold_low * trade_cost\n",
    "        # 未来行情只触发止损，亏损\n",
    "        elif max(ask_price[index: index + look_ahead_period]) >= short_threshold_high * bid_price[index]:\n",
    "            short_profit = 2 - short_threshold_high * trade_cost\n",
    "        # 没有触发止盈止损，计算下最优收益，和下面的做多方向比窘，用来判断市场方向\n",
    "        else: \n",
    "            short_profit = (2 * bid_price[index] - min(ask_price[index: index + look_ahead_period]) * trade_cost) / bid_price[index]\n",
    "\n",
    "        # 做多方向\n",
    "        if max(bid_price[index: index + look_ahead_period]) >= long_threshold_high * ask_price[index] and min(\n",
    "                bid_price[index: index + look_ahead_period]) <= long_threshold_low * ask_price[index]:\n",
    "            if np.where(bid_price[index: index + look_ahead_period] >= long_threshold_high * ask_price[index])[0][0] < \\\n",
    "                    np.where(bid_price[index: index + look_ahead_period] <= long_threshold_low * ask_price[index])[0][0]:\n",
    "                long_profit = long_threshold_high / trade_cost\n",
    "            else:\n",
    "                long_profit = long_threshold_low / trade_cost\n",
    "        elif max(bid_price[index: index + look_ahead_period]) >= long_threshold_high * ask_price[index]:\n",
    "            long_profit = long_threshold_high / trade_cost\n",
    "        elif min(bid_price[index: index + look_ahead_period]) <= long_threshold_low * ask_price[index]:\n",
    "            long_profit = long_threshold_low / trade_cost\n",
    "        else:\n",
    "            long_profit = max(bid_price[index: index + look_ahead_period])/ (ask_price[index] * trade_cost)\n",
    "\n",
    "        # 资产变化超过阈值，生成交易信号\n",
    "        signals.append(np.argmax([short_profit * int(short_profit > 1+spread), long_profit * int(long_profit > 1+spread), 1]))\n",
    "    return pd.Series(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_label(data: pd.DataFrame) -> pd.Series:\n",
    "    bid_price = data.iloc[:, 0]\n",
    "    ask_price = data.iloc[:, 2]\n",
    "    labels = generate_trade_signals(bid_price, ask_price)\n",
    "    return labels\n",
    "\n",
    "\n",
    "length = data.shape[0]\n",
    "\n",
    "train_data = data.iloc[1:int(length*0.8),:].reset_index(drop=True)\n",
    "train_label = get_label(train_data)\n",
    "\n",
    "\n",
    "valid_data = data.iloc[int(length*0.8):int(length*0.9),:].reset_index(drop=True)\n",
    "valid_label = get_label(valid_data)\n",
    "\n",
    "\n",
    "test_data = data.iloc[int(length*0.9):int(length),:].reset_index(drop=True)\n",
    "test_label = get_label(test_data)\n",
    "\n",
    "\n",
    "# 归一化，注意要用train_data的均值和方差，对valid_data和test_data进行归一化，否则会引入未来信息\n",
    "ss = preprocessing.StandardScaler().fit(train_data)\n",
    "train_data = pd.DataFrame(ss.transform(train_data)).values[:-look_ahead_period,:]\n",
    "valid_data = pd.DataFrame(ss.transform(valid_data)).values[:-look_ahead_period,:]\n",
    "test_data  = pd.DataFrame(ss.transform(test_data)).values[:-look_ahead_period,:]\n",
    "\n",
    "\n",
    "# label 过程非常慢，缓存数据，避免重复计算\n",
    "\n",
    "np.save('train_data.npy', train_data)\n",
    "np.save('train_label.npy', train_label)\n",
    "np.save('valid_data.npy', valid_data)\n",
    "np.save('valid_label.npy', valid_label)\n",
    "np.save('test_data.npy', test_data)\n",
    "np.save('test_label.npy', test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('train_data.npy')\n",
    "train_label = np.load('train_label.npy')\n",
    "print(train_label.value_counts())\n",
    "changes = (train_label != train_label.shift()).sum()\n",
    "print(f\"Train label changes: {changes}\")\n",
    "\n",
    "valid_data = np.load('valid_data.npy')\n",
    "valid_label = np.load('valid_label.npy')\n",
    "print(valid_label.value_counts())\n",
    "changes = (valid_label != valid_label.shift()).sum()\n",
    "print(f\"Valid label changes: {changes}\")\n",
    "\n",
    "test_data = np.load('test_data.npy')\n",
    "test_label = np.load('test_label.npy')\n",
    "print(test_label.value_counts())\n",
    "changes = (test_label != test_label.shift()).sum()\n",
    "print(f\"Test label changes: {changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_classification(X, Y, T):\n",
    "    [N, D] = X.shape\n",
    "    df = np.array(X)\n",
    "    dY = np.array(Y)\n",
    "    dataY = dY[T - 1:N]\n",
    "    dataX = np.zeros((N - T + 1, T, D),dtype='float16')\n",
    "    for i in range(T, N + 1):\n",
    "        dataX[i - T] = df[i - T:i, :]\n",
    "    return dataX.reshape(dataX.shape + (1,)), dataY\n",
    "\n",
    "# 用过去T个tick的数据预测，这个参数可以调整，对gpu显存大小有影响，8G显存最多也就60了\n",
    "T = 60\n",
    "trainX_CNN, trainY_CNN = data_classification(train_data, train_label, T)\n",
    "trainY_CNN = keras.utils.to_categorical(trainY_CNN, 3)\n",
    "\n",
    "validX_CNN, validY_CNN = data_classification(valid_data, valid_label, T)\n",
    "validY_CNN = keras.utils.to_categorical(validY_CNN, 3)\n",
    "testX_CNN, testY_CNN = data_classification(test_data, test_label, T)\n",
    "\n",
    "testY_CNN = keras.utils.to_categorical(testY_CNN, 3)\n",
    "del train_data, train_label, valid_data, valid_label,test_data, test_label\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deeplob(T, NF):\n",
    "    input_lmd = keras.layers.Input(shape=(T, NF, 1))\n",
    "\n",
    "    node = 16\n",
    "    negative_slope = 0.01\n",
    "    # build the convolutional block\n",
    "    conv_first1 = keras.layers.Conv2D(node, (1, 2), strides=(1, 2))(input_lmd)\n",
    "    conv_first1 = keras.layers.LeakyReLU(negative_slope=negative_slope)(conv_first1)\n",
    "    conv_first1 = keras.layers.Conv2D(node, (5, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(negative_slope=negative_slope)(conv_first1)\n",
    "    \n",
    "    conv_first1 = keras.layers.Conv2D(node, (1, 2), strides=(1, 2))(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(negative_slope=negative_slope)(conv_first1)\n",
    "    conv_first1 = keras.layers.Conv2D(node, (5, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(negative_slope=negative_slope)(conv_first1)\n",
    "\n",
    "    conv_first1 = keras.layers.Conv2D(node, (1, 5))(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(negative_slope=negative_slope)(conv_first1)\n",
    "    conv_first1 = keras.layers.Conv2D(node, (5, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(negative_slope=negative_slope)(conv_first1)\n",
    "    \n",
    "    node2 = 16\n",
    "    # build the inception module\n",
    "    convsecond_1 = keras.layers.Conv2D(node2, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_1 = keras.layers.LeakyReLU(negative_slope=negative_slope)(convsecond_1)\n",
    "    convsecond_1 = keras.layers.Conv2D(node2, (3, 1), padding='same')(convsecond_1)\n",
    "    convsecond_1 = keras.layers.LeakyReLU(negative_slope=negative_slope)(convsecond_1)\n",
    "\n",
    "    convsecond_2 = keras.layers.Conv2D(node2, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_2 = keras.layers.LeakyReLU(negative_slope=negative_slope)(convsecond_2)\n",
    "    convsecond_2 = keras.layers.Conv2D(node2, (5, 1), padding='same')(convsecond_2)\n",
    "    convsecond_2 = keras.layers.LeakyReLU(negative_slope=negative_slope)(convsecond_2)\n",
    "\n",
    "    convsecond_3 = keras.layers.MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
    "    convsecond_3 = keras.layers.Conv2D(node2, (1, 1), padding='same')(convsecond_3)\n",
    "    convsecond_3 = keras.layers.LeakyReLU(negative_slope=negative_slope)(convsecond_3)\n",
    "\n",
    "    convsecond_output = keras.layers.concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
    "    conv_reshape = keras.layers.Reshape((int(convsecond_output.shape[1]), int(convsecond_output.shape[3])))(convsecond_output)\n",
    "\n",
    "    # build the last LSTM layer\n",
    "    # 这个参数可以调整，对gpu显存大小有影响，8G显存最多也就32了\n",
    "    # 我怎么调这个参数模型效果都差不多，不知道为什么\n",
    "    number_of_lstm = 8\n",
    "    conv_lstm = keras.layers.LSTM(number_of_lstm)(conv_reshape)\n",
    "\n",
    "    # build the output layer\n",
    "    out = keras.layers.Dense(3, activation='softmax')(conv_lstm)\n",
    "    model = keras.models.Model(inputs=input_lmd, outputs=out)\n",
    "    adam = keras.optimizers.Adam()\n",
    "    loss = keras.losses.CategoricalFocalCrossentropy(\n",
    "        alpha=1,  # 平衡因子，用于平衡正负样本\n",
    "        gamma=2  # 焦点因子，用于关注难分类的样本\n",
    "    ),\n",
    "    model.compile(optimizer=adam, loss=loss, metrics=['accuracy'])\n",
    "    # model.compile(optimizer=adam, loss=keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "deeplob = create_deeplob(T, 20)\n",
    "# deeplob.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='auto')\n",
    "checkpoint_filepath = './model_check/sandra.weights.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                            save_weights_only=True,\n",
    "                                                            monitor='val_loss',\n",
    "                                                            mode='auto',\n",
    "                                                            save_best_only=True)\n",
    "deeplob.fit(trainX_CNN, trainY_CNN, epochs=200, batch_size=128, verbose=2, validation_data=(validX_CNN, validY_CNN),\n",
    "            callbacks=[model_checkpoint_callback, early_stopping])  \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainX_CNN, trainY_CNN, validX_CNN, validY_CNN\n",
    "gc.collect()\n",
    "# evaluate the model\n",
    "deeplob.load_weights(checkpoint_filepath)\n",
    "predictions = deeplob.predict(testX_CNN)\n",
    "results = keras.utils.to_categorical(np.argmax(predictions, axis=1), 3)\n",
    "print(classification_report(testY_CNN, results, target_names=['0', '1', '2']))\n",
    "\n",
    "print(\"Evaluate\")\n",
    "result = deeplob.evaluate(testX_CNN, testY_CNN, verbose=0)\n",
    "print(dict(zip(deeplob.metrics_names, result)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
